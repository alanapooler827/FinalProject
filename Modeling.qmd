---
title: "Modeling"
author: "Alana Pooler"
editor: visual
---

## Introduction

## Data Preparation

```{r include=FALSE}
library(tidyverse)
library(tidymodels)
library(ggplot2)
library(vip)

# suppress scientific notation
options(scipen=999)
```

Read in data

```{r}
df <- read_csv("diabetes_binary_health_indicators_BRFSS2015.csv")

# view first few rows of data
head(df)
```

Convert columns to factors

```{r}
# specify columns to skip
skip_convert <- c('BMI', 'MentHlth', 'PhysHlth')

# convert all other columns to factors
df <- df |>
  mutate(across(-all_of(skip_convert), as.factor))
```

Recode factor levels

Recode variables with factor levels of Yes/No

```{r}
# skip variables that are not factors or do not have levels of yes/no 
skip_recode <- c('Diabetes_binary', 'BMI', 'GenHlth', 'MentHlth',
                 'PhysHlth', 'Sex', 'Age', 'Education', 'Income')

lvl_map <- c("0" = "No", "1" = "Yes")

df <- df |>
  mutate(across(-all_of(skip_recode), ~ recode(.x, !!!lvl_map)))
```

Recode remaining variables

```{r}
df <- df |>
  mutate(
    Diabetes_binary = fct_recode(
      Diabetes_binary,
      'None' = '0',
      'Prediabetes/Diabetes' = '1'
    ),
    GenHlth = fct_recode(
      GenHlth,
      'Excellent' = '1',
      'Very Good' = '2',
      'Good' = '3',
      'Fair' = '4',
      'Poor' = '5'
    ),
    Sex = fct_recode(
      Sex,
      'Female' = '0',
      'Male' = '1'
    ),
    Age = fct_recode(
      Age,
      '18-24' = '1',
      '25-29' = '2',
      '30-34' = '3',
      '35-39' = '4',
      '40-44' = '5',
      '45-49' = '6',
      '50-54' = '7',
      '55-59' = '8',
      '60-64' = '9',
      '65-69' = '10',
      '70-74' = '11',
      '75-79' = '12',
      '80+' = '13'
      ),
    Education = fct_recode(
      Education,
      'None' = '1',
      'Elementary School' = '2',
      'Some High School' = '3',
      'High School' = '4',
      'Some College' = '5',
      'College Graduate' = '6',
      ),
    Income = fct_recode(
      Income,
      "Less than $10k" = "1",
      "$10k to < $15k" = "2",
      "$15k to < $20k" = "3",
      "$20k to < $25k" = "4",
      "$25k to < $35k" = "5",
      "$35k to < $50k" = "6",
      "$50k to < $75k" = "7",
      "$75k or more" = "8"
      ),
    )
```

Split data into training and test sets

Set seed for reproducability

```{r}
set.seed(42)
```

Split data

```{r}
# put 70% of the data in the training set
data_split <- initial_split(df, prop = .70)

# create data frames for the two sets
train <- training(data_split)
test <- testing(data_split)
```

Create 5 fold cross validation split on the training data

```{r}
cv_fold <- vfold_cv(train, 5)
```

Create recipes- one with all predictor variables, and one with a subset of predictors based on variable importance

```{r}
rec <- recipe(Diabetes_binary ~ ., data = train)

rec2 <- recipe(Diabetes_binary ~ HighBP + GenHlth + HighChol + Age + BMI + PhysHlth,
              data = train)
```

## Classification Tree

A classification tree is a type of predictive model used when the response variable is categorical, like Diabetes_binary. It works by splitting the predictor space up into regions, making a prediction based on which bin an observation ends up in, and then using the most prevalent class in a bin as the prediction. Classification trees are flexible and easy to interpret.

```{r}
tree <- 
  decision_tree(
    tree_depth = tune(),
    min_n = 30,
    cost_complexity = tune()) |>
  set_engine('rpart') |>
  set_mode('classification')

# workflow
tree_wkf <- workflow() |>
  add_recipe(rec) |>
  add_model(tree)
```

Fit the workflow on CV folds

Use 5 of each tuning parameter (cost_complexity and min_n)

```{r}
tree_fit <- tree_wkf |> 
  tune_grid(resamples = cv_fold,
            grid = grid_regular(tree_depth(),
                                cost_complexity(),
                                levels = c(5,5)),
            metrics = metric_set(accuracy, mn_log_loss))

# view metrics for each fold
tree_fit |>
  collect_metrics(type = 'wide') |>
  arrange(mn_log_loss, accuracy)
```

Select the best model based on log loss and finalize the model on the full training set

```{r}
# choose the best model based on log loss
best_tree <- tree_fit |>
  select_best(metric = 'mn_log_loss')

# finalize model
final_wkf <- finalize_workflow(tree_wkf, best_tree)

# fit the finalized model
final_fit <- final_wkf |>
  fit(data = train)
```

Look at variable importance

```{r}
vip(final_fit)
```

## Random Forest

A random forest model is a type of ensemble model. It uses bootstrap sampling to create several smaller data sets from the original data set, and then fits a tree on each of those samples. Instead of using all predictors for every model, each model uses a different random subset of predictors. Final predictions are created by using the most prevalant prediction made by each tree.

Using several trees to make predictions instead of a single tree, as a classification tree does, can improve prediction accuracy.

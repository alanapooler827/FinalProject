---
title: "Modeling"
author: "Alana Pooler"
editor: visual
---

# Introduction

In this section, we will be creating two predictive models using the 2015 Behavioral Risk Factor Surveillance System report data. The goal is to use the predictor variables in the data set to predict the variable Diabetes_binary, which indicates whether or not each subject has diabetes.

We will build a classification tree and a random forest model and evaluate both to determine which model performs the best at predicting the variable Diabetes_binary.

# Data Preparation

```{r}
#| output: false
library(tidyverse)
library(tidymodels)
library(ggplot2)
library(future)

# import function to load and clean the data set
source("data/load_clean_data.R")

# suppress scientific notation
options(scipen=999)

# Use 10 cores
plan(multisession, workers = 10)
```

Import cleaned data

```{r}
df <- load_clean_data()

# view first few rows of data
head(df)
```

Split data into training and test sets

Set seed for reproducibility

```{r}
set.seed(42)
```

Split the data using 70% for the training set and 30% for the test set.

```{r}
# put 70% of the data in the training set
data_split <- initial_split(df, prop = .70)

# create data frames for the two sets
train <- training(data_split)
test <- testing(data_split)
```

Create 5 fold cross validation split on the training data

```{r}
cv_fold <- vfold_cv(train, 5)
```

Create recipes

The subset of variables used in recipe 2 were chosen by fitting the model with all 21 predictor variables and looking at variable importance.

```{r}
# recipe using all 21 predictor variables
rec <- recipe(Diabetes_binary ~ ., data = train)

# recipe using subset of predictor variables
rec2 <- recipe(Diabetes_binary ~ HighBP + GenHlth + HighChol +
                 Age + BMI + PhysHlth + DiffWalk,
               data = train)
```

------------------------------------------------------------------------

# Classification Tree

A classification tree is a type of predictive model used when the response variable is categorical, like Diabetes_binary. It works by splitting the predictor space up into regions, making a prediction based on which bin an observation ends up in, and then using the most prevalent class in a bin as the prediction. Classification trees are flexible and easy to interpret.

**Create classification tree model instance and workflow**

Set `cost_complexity = tune()` to find the optimal value for that parameter.

```{r}
tree_spec <- decision_tree(tree_depth = 8,
                           min_n = 2,
                           cost_complexity = tune()) |>
  set_engine('rpart') |>
  set_mode('classification')

# workflow
tree_wkf <- workflow() |>
  add_recipe(rec2) |>
  add_model(tree_spec)
```

Fit the workflow on CV folds

Try 5 values of the tuning parameter `cost_complexity`

```{r}
tree_fit <- tree_wkf |> 
  tune_grid(resamples = cv_fold,
            grid = grid_regular(cost_complexity(),
                                levels = 5),
            metrics = metric_set(mn_log_loss, accuracy))

# view metrics for each fold
tree_fit |>
  collect_metrics(type = 'wide') |>
  arrange(mn_log_loss, accuracy)
```

Select the best model based on log loss

```{r}
# choose the best model based on log loss
best_tree <- tree_fit |>
  select_best(metric = 'mn_log_loss')
best_tree
```

------------------------------------------------------------------------

# Random Forest

A random forest model is a type of ensemble model. It uses bootstrap sampling to create several smaller data sets from the original data set, and then fits a tree on each of those samples. Instead of using all predictors for every model, each model uses a different random subset of predictors. Final predictions are created by using the most prevalent prediction made by each tree.

Using several trees to make predictions instead of a single tree, as a classification tree does, can improve prediction accuracy.

**Create Random Forest model instance and workflow**

Set `mtry = tune()` to find the optimal value for that parameter

Using all 21 predictors results in better metrics than selecting a subset of predictors based on variable importance, so we will use the first recipe for this model

```{r}
rf_spec <- 
  rand_forest(mtry = tune(),
              min_n = 40,
              trees = 100) |>
  set_engine('ranger') |>
  set_mode('classification')

rf_wkf <- workflow() |>
  add_recipe(rec) |>
  add_model(rf_spec)
```

Fit model to CV folds and view metrics

```{r}
rf_fit <- rf_wkf |>
  tune_grid(
    resamples = cv_fold,
    grid = 5,
    metrics = metric_set(mn_log_loss, accuracy))

rf_fit |>
  collect_metrics(type = 'wide') |>
  arrange(mn_log_loss, accuracy)
```

Select the best model based on log loss

```{r}
# choose the best model based on log loss
best_rf <- rf_fit |>
  select_best(metric = 'mn_log_loss')
best_rf
```

------------------------------------------------------------------------

# Model Comparison

Fit each model on the test set

Classification Tree

```{r}
tree_final <- tree_wkf |>
  finalize_workflow(best_tree) |>
  last_fit(data_split, metrics = metric_set(mn_log_loss, accuracy))

tree_final |>
  collect_metrics()
```

Random Forest Model

```{r}
rf_final <- rf_wkf |>
  finalize_workflow(best_rf) |>
  last_fit(data_split, metrics = metric_set(mn_log_loss, accuracy))

rf_final |>
  collect_metrics()
```

Compare metrics for each model

The random forest model has a very slightly higher accuracy and lower log loss than the classification tree model, so random forest is crowned the winner!

```{r}
rbind(
  tree_final |>
    collect_metrics(type = 'wide') |> 
    mutate(Model = 'TREE', .before = 'mn_log_loss'),
  rf_final |>
    collect_metrics(type = 'wide') |> 
    mutate(Model = 'RF', .before = 'mn_log_loss')
) |>
  arrange(mn_log_loss, accuracy)
```
